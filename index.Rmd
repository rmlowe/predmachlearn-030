---
title: "Practical Machine Learning â€” Prediction Assignment Writeup"
author: "Robert Lowe"
date: "20 July 2015"
output: html_document
---

First we load the data (omitting row numbers, which will confuse the model since the observations are sorted by outcome).

```{r}
pmlTraining <- read.csv(file = "pml-training.csv", na.strings = c("NA", "#DIV/0!"))[, 2:160]
pmlTesting <- read.csv(file = "pml-testing.csv", na.strings = c("NA", "#DIV/0!"))[, 2:160]
```

From visual inspection of the data it's apparent that many variables have mostly `NA` values. Let's look at the proportion of `NA` values for each variable.

```{r}
avgNA <- apply(pmlTraining, 2, function(v) { sum(is.na(v)) / nrow(pmlTraining)  })
avgNA
```

We'll remove variables with more than 90% of the values missing.

```{r}
filteredTrain <- pmlTraining[, avgNA <= 0.9]
filteredTest <- pmlTesting[, avgNA <= 0.9]
```

Next we create dummy variables from the factors.

```{r}
library(caret)
merged <- rbind(filteredTest[, -59], filteredTrain[, -59])
dummies <- dummyVars(~ ., data = merged)
dv <- as.data.frame(predict(dummies, newdata = merged))
pmldv <- dv[21:nrow(dv), ]
pmldvTest <- dv[1:20, ]
```

We will ignore variables with near zero-variance.

```{r}
nzv <- nearZeroVar(pmldv)
filteredPredictors <- pmldv[, -nzv]
filteredPredictorsTest <- pmldvTest[, -nzv]
```

Now create training and test sets.

```{r}
set.seed(745363615)
inTrain <- createDataPartition(y = pmlTraining$classe, p = 0.75, list = FALSE)
trainPredictors <- filteredPredictors[inTrain, ]
cvPredictors <- filteredPredictors[-inTrain, ]
```

We plot the observations in the training set against the first two principal components. We find clear clusters in the observations, however these clusters don't appear to correspond with the exercise classes.

```{r}
preProcValues <- preProcess(x = trainPredictors, method = "pca", pcaComp = 2)
trainPCA <- predict(preProcValues, trainPredictors)
plotdata <- data.frame(trainPCA,
                       classe = pmlTraining$classe[inTrain],
                       user_name = pmlTraining$user_name[inTrain])
ggplot(data = plotdata, mapping = aes(x = PC1, y = PC2, colour = classe)) + geom_point()
```

The plot below shows that the clusters actually correspond with `user_name`.

```{r}
ggplot(data = plotdata, mapping = aes(x = PC1, y = PC2, colour = user_name)) + geom_point()
```

So it appears fairly straightforward to distinguish the observations based on `user_name`, but it might be harder distinguish the observations based on exercise class.

We fit a CART model.

```{r}
training <- data.frame(trainPredictors, classe = pmlTraining$classe[inTrain])
cv <- data.frame(cvPredictors, classe = pmlTraining$classe[-inTrain])
cart <- train(form = classe ~ ., data = training, method = "rpart")
cart$finalModel
plot(x = cart$finalModel, uniform = TRUE, main = "Classification Tree")
text(x = cart$finalModel, use.n = TRUE, all = TRUE, cex = 0.8)
library(rattle)
fancyRpartPlot(cart$finalModel)
trainCM <- confusionMatrix(data = predict(cart, newdata = training), reference = training$classe)
trainCM
cvCM <- confusionMatrix(data = predict(cart, newdata = cv), reference = cv$classe)
cvCM
```

The accuracy in the training set is only `r signif(x = trainCM$overall[1], digits = 3)`, and `r signif(x = cvCM$overall[1], digits = 3)` in the cross-validation set. Hopefully we can do better.

Let's try a random forest.

```{r}
samp <- training[sample.int(n = nrow(training), size = 0.2 * nrow(training)), ]
forest <- train(form = classe ~ ., data = samp, method = "rf")
trainCM <- confusionMatrix(data = predict(forest, newdata = training), reference = training$classe)
trainCM
cvCM <- confusionMatrix(data = predict(forest, newdata = cv), reference = cv$classe)
cvCM
```

The accuracy in the training set is `r signif(x = trainCM$overall[1], digits = 3)`, and `r signif(x = cvCM$overall[1], digits = 3)` in the cross-validation set.

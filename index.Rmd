---
title: "Practical Machine Learning â€” Prediction Assignment Writeup"
author: "Robert Lowe"
date: "20 July 2015"
output: html_document
---

### Loading the data

First we load the data (omitting row numbers in the first column, which will confuse the model since the observations are sorted by outcome). Many cells contain the text `#DIV/0!`; we treat these as missing values.

```{r}
pmlTraining <- read.csv(file = "pml-training.csv", na.strings = c("NA", "#DIV/0!"))[, 2:160]
pmlTesting <- read.csv(file = "pml-testing.csv", na.strings = c("NA", "#DIV/0!"))[, 2:160]
```

From visual inspection of the data it's apparent that many variables have mostly `NA` values. Let's look at the proportion of `NA` values for each variable.

```{r}
avgNA <- apply(pmlTraining, 2, function(v) { mean(is.na(v))  })
avgNA
```

We'll remove variables with more than 90% of the values missing.

```{r}
filteredTrain <- pmlTraining[, avgNA <= 0.9]
filteredTest <- pmlTesting[, avgNA <= 0.9]
```

### Creating dummy variables

Next we create dummy (numeric) variables from the factors.

```{r, message=FALSE}
library(caret)
merged <- rbind(filteredTest[, -59], filteredTrain[, -59])
dummies <- dummyVars(~ ., data = merged)
dv <- as.data.frame(predict(dummies, newdata = merged))
pmldv <- dv[21:nrow(dv), ]
pmldvTest <- dv[1:20, ]
```

### Predictors with near zero-variance

We will ignore variables with near zero-variance.

```{r}
nzv <- nearZeroVar(pmldv)
filteredPredictors <- pmldv[, -nzv]
filteredPredictorsTest <- pmldvTest[, -nzv]
```

### Splitting the data

Now create training and cross-validation sets, preserving the overall class distribution.

```{r}
set.seed(745363615)
inTrain <- createDataPartition(y = pmlTraining$classe, p = 0.75, list = FALSE)
trainPredictors <- filteredPredictors[inTrain, ]
cvPredictors <- filteredPredictors[-inTrain, ]
```

### Principal components

We plot the observations in the training set against the first two principal components. We find clear clusters in the observations, however these clusters don't appear to correspond with the exercise classes.

```{r}
preProcValues <- preProcess(x = trainPredictors, method = "pca", pcaComp = 2)
trainPCA <- predict(preProcValues, trainPredictors)
plotdata <- data.frame(trainPCA,
                       classe = pmlTraining$classe[inTrain],
                       user_name = pmlTraining$user_name[inTrain])
ggplot(data = plotdata, mapping = aes(x = PC1, y = PC2, colour = classe)) + geom_point()
```

The plot below shows that the clusters actually correspond with `user_name`.

```{r}
ggplot(data = plotdata, mapping = aes(x = PC1, y = PC2, colour = user_name)) + geom_point()
```

So it appears fairly straightforward to distinguish the observations based on `user_name`, but it might be harder distinguish the observations based on exercise class.

### A CART model

We fit a CART model.

```{r, message=FALSE, warning=FALSE}
training <- data.frame(trainPredictors, classe = pmlTraining$classe[inTrain])
cv <- data.frame(cvPredictors, classe = pmlTraining$classe[-inTrain])
cart <- train(form = classe ~ ., data = training, method = "rpart")
library(rattle)
fancyRpartPlot(cart$finalModel)
```
```{r}
trainCM <- confusionMatrix(data = predict(cart, newdata = training), reference = training$classe)
trainCM
cvCM <- confusionMatrix(data = predict(cart, newdata = cv), reference = cv$classe)
cvCM
```

The in sample error for the CART model is only `r signif(x = trainCM$overall[1], digits = 3)`, and the out of sample error (as determined by cross-validation) is `r signif(x = cvCM$overall[1], digits = 3)`. Hopefully we can do better.

### A random forest

Let's try a random forest.

```{r, include=FALSE}
library(randomForest)
```
```{r}
# This takes several hours, so used cached model if available
getForest <- function(name) {
  if (file.exists(name)) {
    readRDS(file = name)
  } else {
    result <- train(form = classe ~ ., data = training, method = "rf")
    saveRDS(object = result, file = name)
    result
  }
}
forest <- getForest("forest.rds")
trainCM <- confusionMatrix(data = predict(forest, newdata = training), reference = training$classe)
trainCM
cvCM <- confusionMatrix(data = predict(forest, newdata = cv), reference = cv$classe)
cvCM
```

Thus the in sample error for the random forest is `r signif(x = trainCM$overall[1], digits = 3)`, and the out of sample error (as determined by cross-validation) is `r signif(x = cvCM$overall[1], digits = 3)`. So the random forest is clearly greatly superior to the simple CART model for this prediction task.
